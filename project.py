# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r3Uyr3kZYXS1wWsStS2OCjy8cVd7vJaN

# **Text Summarization with Encoder-Decoder and Attention**

## **1. Pendahuluan**

Proyek ini bertujuan untuk membangun text summarization berbasis **arsitektur Encoder-Decoder** yang dilengkapi dengan mekanisme **Attention LSTM**.

Dataset yang digunakan berupa **ulasan dari Universitas Bengkulu beserta ringkasannya**.
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import pickle
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from bs4 import BeautifulSoup
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, Attention
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
import warnings
warnings.filterwarnings("ignore")
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Configurations
warnings.filterwarnings("ignore")
nltk.download('punkt')
nltk.download('stopwords')

"""## **2. Data Preprocessing**
Dataset ini terdiri dari dua kolom: `text` (ulasan lengkap) dan `summary` (ringkasan ulasan).  

Data akan dibersihkan dari karakter yang tidak diperlukan, ditokenisasi, dan dipersiapkan untuk model.

### **Load Dataset**
"""

# Dataset is a collection of reviews and summaries.
url = "https://github.com/search01/UAS_DeepLearning/blob/main/Dataset.xlsx"
df = pd.read_excel(url)

df.head()

df.drop_duplicates(subset=['text'], inplace=True)
df.dropna(axis=0, inplace=True)

print("Dataset loaded successfully.")
print(f"Number of samples: {len(df)}")

"""### **Text Cleaning**
- Menghapus tag HTML menggunakan **BeautifulSoup**.  
- Melakukan tokenisasi teks dan menyaring token yang bukan alfanumerik.  
- Menghilangkan *stopwords* untuk memfokuskan pada kata-kata yang lebih bermakna.
"""

stop_words = set(stopwords.words('indonesian'))

def clean_text(text, is_target=False):
    """Clean text for preprocessing."""
    text = BeautifulSoup(text, "lxml").text  # Remove HTML
    words = word_tokenize(text.lower())  # Tokenize
    words = [word for word in words if word.isalpha() and word not in stop_words]
    if is_target:
        return "sos " + " ".join(words) + " eos"
    return " ".join(words)

df['cleaned_text'] = df['text'].apply(clean_text)
df['cleaned_summary'] = df['summary'].apply(lambda x: clean_text(x, is_target=True))

# Split into training and testing datasets
x_train, x_test, y_train, y_test = train_test_split(
    df['cleaned_text'], df['cleaned_summary'], test_size=0.1, random_state=42
)

"""Mengubah teks menjadi urutan bilangan bulat, di mana setiap kata direpresentasikan oleh indeks dalam kosakata.  
- **Padding** digunakan untuk memastikan semua urutan memiliki panjang yang sama dengan menambahkan angka nol pada urutan yang lebih pendek.
"""

# Tokenization and Padding
input_tokenizer = Tokenizer()
target_tokenizer = Tokenizer()

input_tokenizer.fit_on_texts(x_train)
target_tokenizer.fit_on_texts(y_train)

x_train_seq = input_tokenizer.texts_to_sequences(x_train)
y_train_seq = target_tokenizer.texts_to_sequences(y_train)

max_in_len = max(len(seq) for seq in x_train_seq)
max_out_len = max(len(seq) for seq in y_train_seq)

x_train_padded = pad_sequences(x_train_seq, maxlen=max_in_len, padding='post')
y_train_padded = pad_sequences(y_train_seq, maxlen=max_out_len, padding='post')

# Decoder input and target preparation
decoder_input_data = y_train_padded[:, :-1]
decoder_target_data = y_train_padded[:, 1:]

# Vocabulary sizes
input_vocab_size = len(input_tokenizer.word_index) + 1
target_vocab_size = len(target_tokenizer.word_index) + 1

"""## **3. Model Definition**
Model Encoder-Decoder akan dibangun dengan mekanisme perhatian (*attention*).  
- **Encoder** memproses urutan input dan menghasilkan *context vector*.  
- **Decoder** menghasilkan urutan output secara bertahap, satu kata dalam satu waktu.
"""

latent_dim = 256

# Encoder
encoder_inputs = Input(shape=(max_in_len,))
encoder_embedding = Embedding(input_vocab_size, latent_dim)(encoder_inputs)
encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(target_vocab_size, latent_dim)(decoder_inputs)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, *_ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])

# Attention Mechanism
attention_layer = Attention()
attention_outputs = attention_layer([decoder_outputs, encoder_outputs])
concat_outputs = Concatenate(axis=-1)([decoder_outputs, attention_outputs])

# Dense Layer
decoder_dense = Dense(target_vocab_size, activation='softmax')
final_outputs = decoder_dense(concat_outputs)

# Define Model
from tensorflow.keras.utils import plot_model
model = Model([encoder_inputs, decoder_inputs], final_outputs)
model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

"""## **4. Training Model**
Model dilatih menggunakan dataset pelatihan dan divalidasi dengan membagi data menjadi bagian validasi.

**EarlyStopping** akan digunakan untuk mencegah *overfitting*.
"""

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    [x_train_padded, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=100,
    validation_split=0.2,
    callbacks=[early_stopping]
)

# Save the model and tokenizers
model.save("text_summarizer_model.h5")
with open("input_tokenizer.pkl", "wb") as f:
    pickle.dump(input_tokenizer, f)
with open("target_tokenizer.pkl", "wb") as f:
    pickle.dump(target_tokenizer, f)

"""## **5. Evaluation**
Kurva model pada training dan validation akurasi dan loss

"""

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title("Accuracy")
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title("Loss")
plt.show()

"""## **6. Inference**"""

# Load the model for inference
from tensorflow.keras.models import load_model
model = load_model("text_summarizer_model.h5")

# Encoder model
encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])

# Decoder model
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_hidden_state_input = Input(shape=(max_in_len, latent_dim))
decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]
)
attention_outputs = attention_layer([decoder_outputs, decoder_hidden_state_input])
concat_outputs = Concatenate(axis=-1)([decoder_outputs, attention_outputs])
decoder_outputs = decoder_dense(concat_outputs)
decoder_model = Model(
    [decoder_inputs, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],
    [decoder_outputs, state_h, state_c]
)

# Generate Summary
def generate_summary(input_text):
    input_seq = input_tokenizer.texts_to_sequences([input_text])
    input_seq_padded = pad_sequences(input_seq, maxlen=max_in_len, padding='post')
    encoder_out, state_h, state_c = encoder_model.predict(input_seq_padded)

    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = target_tokenizer.word_index['sos']
    stop_condition = False
    summary = ""

    while not stop_condition:
        decoder_out, state_h, state_c = decoder_model.predict(
            [target_seq, encoder_out, state_h, state_c]
        )
        word_index = np.argmax(decoder_out[0, -1, :])
        word = target_tokenizer.index_word[word_index]
        if word == "eos" or len(summary.split()) >= max_out_len:
            stop_condition = True
        else:
            summary += word + " "
            target_seq[0, 0] = word_index
    return summary.strip()

# Test with an example
test_text = """
Tolong dech tingkatkan lagi pelayanan publiknya...
Tingkatkan Pelayanan adm nya,spy lbh ramah dan solutif...
Trs trg gak cocok bgt yg lg buru2 tp disuruh Nunda Ampe berhari2.
Tempat sholatnya ,air gak ada. Pdhal Suasana kampus enak bgt,bs JD tempat healing,tempat jajan,makan byk pilihan.
"""
cleaned_text = clean_text(test_text)
print("Summary:", generate_summary(cleaned_text))